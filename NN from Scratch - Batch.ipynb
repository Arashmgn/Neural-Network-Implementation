{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0722f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from mlp import MLP\n",
    "from loss import mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d26e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = load_iris()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert the training and testing sets into input and labels\n",
    "train_input = X_train\n",
    "test_input = X_test\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both the training and testing data\n",
    "train_input = scaler.fit_transform(train_input)\n",
    "test_input = scaler.transform(test_input)\n",
    "\n",
    "\n",
    "# One-hot encode the target labels\n",
    "encoder = OneHotEncoder()\n",
    "train_labels = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "test_labels = encoder.transform(y_test.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(4, layer_dims = [64, 64, 3], activationfuncs=[ \"relu\", \"softmax\"], learning_rate=0.0004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse loss :  0.10159912732958372\n",
      "mse loss :  0.10159863545111958\n",
      "mse loss :  0.10159815099352716\n",
      "mse loss :  0.1015976721476323\n",
      "mse loss :  0.10159719706911101\n",
      "mse loss :  0.10159672587513681\n",
      "mse loss :  0.10159625575000998\n",
      "mse loss :  0.10159578292195044\n",
      "mse loss :  0.1015953048931716\n",
      "mse loss :  0.10159481880370241\n",
      "mse loss :  0.1015943213064165\n",
      "mse loss :  0.10159380839881023\n",
      "mse loss :  0.10159327519137738\n",
      "mse loss :  0.10159271558132026\n",
      "mse loss :  0.10159212178176193\n",
      "mse loss :  0.10159148362458453\n",
      "mse loss :  0.10159078749774965\n",
      "mse loss :  0.10159001467134411\n",
      "mse loss :  0.10158913855846659\n",
      "mse loss :  0.10158812002756297\n",
      "mse loss :  0.10158689893577212\n",
      "mse loss :  0.10158537778991675\n",
      "mse loss :  0.1015833874663431\n",
      "mse loss :  0.10158060698092541\n",
      "mse loss :  0.10157634529208978\n",
      "mse loss :  0.1015688001541215\n",
      "mse loss :  0.101551393265369\n",
      "mse loss :  0.10146906990180067\n",
      "mse loss :  0.08288624720338168\n",
      "mse loss :  0.08263888770513587\n",
      "mse loss :  0.08255641204754369\n",
      "mse loss :  0.08043796346286251\n",
      "mse loss :  0.06999059930641699\n",
      "mse loss :  0.06991713184213853\n",
      "mse loss :  0.06988385105176727\n",
      "mse loss :  0.06985714138390897\n",
      "mse loss :  0.06980406687282062\n",
      "mse loss :  0.06385914339666421\n",
      "mse loss :  0.06363618982081347\n",
      "mse loss :  0.06358672925164358\n",
      "mse loss :  0.06356460045078197\n",
      "mse loss :  0.06355164011206318\n",
      "mse loss :  0.06354291558357983\n",
      "mse loss :  0.06353653733612274\n",
      "mse loss :  0.06353161377712403\n",
      "mse loss :  0.06352766092763477\n",
      "mse loss :  0.06352438892762456\n",
      "mse loss :  0.06352161130913571\n",
      "mse loss :  0.06351920142245292\n",
      "mse loss :  0.06351706941023527\n",
      "mse loss :  0.06351514907217877\n",
      "mse loss :  0.06351338987956298\n",
      "mse loss :  0.0635117518382326\n",
      "mse loss :  0.06351020199347955\n",
      "mse loss :  0.06350871189499291\n",
      "mse loss :  0.06350725560102206\n",
      "mse loss :  0.06350580792706603\n",
      "mse loss :  0.0635043426897102\n",
      "mse loss :  0.06350283067690873\n",
      "mse loss :  0.0635012369805092\n",
      "mse loss :  0.06349951710768487\n",
      "mse loss :  0.0634976108278618\n",
      "mse loss :  0.06349543172927972\n",
      "mse loss :  0.06349284824431943\n",
      "mse loss :  0.06348964651260632\n",
      "mse loss :  0.06348545099922595\n",
      "mse loss :  0.06347953502199949\n",
      "mse loss :  0.06347029874744534\n",
      "mse loss :  0.06345353129083396\n",
      "mse loss :  0.06341524299611974\n",
      "mse loss :  0.063315357960243\n",
      "mse loss :  0.06173763804960477\n",
      "mse loss :  0.06029046376220166\n",
      "mse loss :  0.058609743123210614\n",
      "mse loss :  0.0579253278191761\n",
      "mse loss :  0.05764628147665195\n",
      "mse loss :  0.057491017484966975\n",
      "mse loss :  0.057380633736800975\n",
      "mse loss :  0.05727688717619587\n",
      "mse loss :  0.05727589311490736\n",
      "mse loss :  0.05186998849276247\n",
      "mse loss :  0.05117940758690133\n",
      "mse loss :  0.05103251843808676\n",
      "mse loss :  0.050967312255950144\n",
      "mse loss :  0.050929096025457116\n",
      "mse loss :  0.05090419596444344\n",
      "mse loss :  0.050886605660017174\n",
      "mse loss :  0.05087347965044405\n",
      "mse loss :  0.050863285756082274\n",
      "mse loss :  0.050855122375878886\n",
      "mse loss :  0.050848422994094956\n",
      "mse loss :  0.05084281336552593\n",
      "mse loss :  0.050838035856563143\n",
      "mse loss :  0.050833907206345896\n",
      "mse loss :  0.05083029323522423\n",
      "mse loss :  0.05082709337330622\n",
      "mse loss :  0.050824230567027444\n",
      "mse loss :  0.05082164463583906\n",
      "mse loss :  0.050819287717640674\n",
      "mse loss :  0.05081712107247193\n"
     ]
    }
   ],
   "source": [
    "epoch=1000\n",
    "batch_size = 1\n",
    "for i in range(epoch):\n",
    "    outs = []\n",
    "    for j in range(0,len(train_input), batch_size):\n",
    "        x = train_input[j:j+batch_size]\n",
    "        y = train_labels[j:j+batch_size]\n",
    "        out = mlp(x.T)\n",
    "        mlp.backpropagation(y.T, out)\n",
    "        outs.append(out)\n",
    "    if i%10 == 0:\n",
    "        print(\"mse loss : \", mse_loss(train_labels, np.array(outs).reshape([105,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f199ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs= []\n",
    "for input, label in zip(train_input, y_train):\n",
    "    out = mlp(input.reshape(4,1))\n",
    "    outs.append(np.argmax(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4f6190a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1,\n",
       "       2, 0, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 2, 0, 2, 0, 0,\n",
       "       1, 1, 2, 1, 1, 2, 1, 1, 0, 1, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1, 1,\n",
       "       2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 2, 2, 0, 1, 2, 2, 0,\n",
       "       2, 0, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1, 1, 0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(outs).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2404a383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,\n",
       "       -1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
       "        0,  0,  0], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train-outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  1, -1,  0,  1,  0,  0,  0,  0,  1,  0,\n",
       "        0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs= []\n",
    "for input, label in zip(test_input, y_test):\n",
    "    out = mlp(input.reshape(4,1))\n",
    "    # print(out)\n",
    "    outs.append(np.argmax(out))\n",
    "\n",
    "\n",
    "y_test-outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59894727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142, 3)\n",
      "mse loss :  0.23055273508753207\n",
      "mse loss :  0.22200764630136868\n",
      "mse loss :  0.2158174880009909\n",
      "mse loss :  0.21188971762936137\n",
      "mse loss :  0.20959638888890297\n",
      "mse loss :  0.20817012769486504\n",
      "mse loss :  0.20711280812443603\n",
      "mse loss :  0.2061849264087133\n",
      "mse loss :  0.2052863884236396\n",
      "mse loss :  0.20437646338107487\n",
      "mse loss :  0.2034368928611217\n",
      "mse loss :  0.2024573226314541\n",
      "mse loss :  0.20142992448067673\n",
      "mse loss :  0.20034744060915813\n",
      "mse loss :  0.19920244029327533\n",
      "mse loss :  0.1979869936057387\n",
      "mse loss :  0.19669248755681598\n",
      "mse loss :  0.1953094961499617\n",
      "mse loss :  0.19382768217031868\n",
      "mse loss :  0.19223573156082013\n",
      "mse loss :  0.1905213282547835\n",
      "mse loss :  0.18867118130285104\n",
      "mse loss :  0.18667112780877496\n",
      "mse loss :  0.1845063626873139\n",
      "mse loss :  0.18216188716608062\n",
      "mse loss :  0.1796233036514625\n",
      "mse loss :  0.17687809038815783\n",
      "mse loss :  0.17391745609864556\n",
      "mse loss :  0.17073879702021127\n",
      "mse loss :  0.16734858860242421\n",
      "mse loss :  0.1637651244004797\n",
      "mse loss :  0.16001993749907134\n",
      "mse loss :  0.15615649444533905\n",
      "mse loss :  0.15222547400933742\n",
      "mse loss :  0.14827761817829335\n",
      "mse loss :  0.14435664493187522\n",
      "mse loss :  0.14049470411580553\n",
      "mse loss :  0.13671132867497157\n",
      "mse loss :  0.13301513894321623\n",
      "mse loss :  0.1294068611397374\n",
      "mse loss :  0.12588252638093328\n",
      "mse loss :  0.12243634672309249\n",
      "mse loss :  0.11906316743333865\n",
      "mse loss :  0.11576039993925301\n",
      "mse loss :  0.11252905569611084\n",
      "mse loss :  0.10937328548956286\n",
      "mse loss :  0.10629822637427722\n",
      "mse loss :  0.10330705718996926\n",
      "mse loss :  0.10039892439727384\n",
      "mse loss :  0.09756874359277531\n",
      "mse loss :  0.0948084644304587\n",
      "mse loss :  0.09210870709227914\n",
      "mse loss :  0.0894600089004299\n",
      "mse loss :  0.0868535054380556\n",
      "mse loss :  0.0842811971804152\n",
      "mse loss :  0.08173601597880503\n",
      "mse loss :  0.07921185800124327\n",
      "mse loss :  0.07670368666592453\n",
      "mse loss :  0.07420775825070558\n",
      "mse loss :  0.07172198146633774\n",
      "mse loss :  0.0692463816172927\n",
      "mse loss :  0.06678359526401272\n",
      "mse loss :  0.06433927604513051\n",
      "mse loss :  0.061922260173566564\n",
      "mse loss :  0.059544342327934346\n",
      "mse loss :  0.057219568458015735\n",
      "mse loss :  0.05496306420707226\n",
      "mse loss :  0.0527895618286606\n",
      "mse loss :  0.0507119150442132\n",
      "mse loss :  0.04873994224867158\n",
      "mse loss :  0.04687987367765573\n",
      "mse loss :  0.04513449991836605\n",
      "mse loss :  0.043503879778601415\n",
      "mse loss :  0.041986254864499216\n",
      "mse loss :  0.040578747187306155\n",
      "mse loss :  0.03927757671010472\n",
      "mse loss :  0.0380778845459811\n",
      "mse loss :  0.03697352746293695\n",
      "mse loss :  0.035957165820030175\n",
      "mse loss :  0.03502067694698671\n",
      "mse loss :  0.034155699727822175\n",
      "mse loss :  0.033354105526857254\n",
      "mse loss :  0.03260830138832212\n",
      "mse loss :  0.03191137079948629\n",
      "mse loss :  0.03125710005909295\n",
      "mse loss :  0.030639940712485634\n",
      "mse loss :  0.030054945675685184\n",
      "mse loss :  0.02949770256401696\n",
      "mse loss :  0.028964276701088015\n",
      "mse loss :  0.028451168248632502\n",
      "mse loss :  0.027955281711323828\n",
      "mse loss :  0.027473900961769316\n",
      "mse loss :  0.027004659123166835\n",
      "mse loss :  0.026545491419906135\n",
      "mse loss :  0.026094562103557845\n",
      "mse loss :  0.02565016431935241\n",
      "mse loss :  0.025210602337118745\n",
      "mse loss :  0.024774074723374315\n",
      "mse loss :  0.024338580818275776\n",
      "mse loss :  0.023901870601818687\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the wine dataset\n",
    "data = load_wine()\n",
    "X = data.data   # Features (inputs)\n",
    "y = data.target # Target (outputs)\n",
    "\n",
    "# Split the dataset into a training set and a test set\n",
    "# test_size=0.2 means 20% of data is used for testing and 80% for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, X_train and y_train are the training inputs and outputs,\n",
    "# and X_test and y_test are the testing inputs and outputs.\n",
    "train_input = X_train\n",
    "test_input = X_test\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both the training and testing data\n",
    "train_input = scaler.fit_transform(train_input)\n",
    "test_input = scaler.transform(test_input)\n",
    "\n",
    "# One-hot encode the target labels\n",
    "encoder = OneHotEncoder()\n",
    "train_labels = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "test_labels = encoder.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "print(train_labels.shape)\n",
    "\n",
    "epoch=1000\n",
    "\n",
    "# mlp = MLP(13, layer_dims = [128, 64, 3], activationfuncs=[\"softmax\", \"softmax\"], learning_rate=0.05)\n",
    "mlp = MLP(13, layer_dims = [128, 64, 3], activationfuncs=[\"softmax\", \"softmax\"], learning_rate=0.005)\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    outs = []\n",
    "    for x, y in zip(train_input, train_labels):\n",
    "        out = mlp(x.reshape(13,1))\n",
    "        mlp.backpropagation(y.reshape(3,1), out)\n",
    "        outs.append(out)\n",
    "    if i%10 == 0:\n",
    "        print(\"mse loss : \", mse_loss(train_labels, np.array(outs).reshape([142,3])))\n",
    "\n",
    "outs= []\n",
    "for input, label in zip(train_input, y_train):\n",
    "    out = mlp(input.reshape(13,1))\n",
    "    # print(out)\n",
    "    outs.append(np.argmax(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c444a8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 2, 1, 1, 1, 1, 2, 0, 1, 1, 2, 0, 1, 0, 0, 2, 2, 1, 1, 0,\n",
       "       1, 0, 2, 1, 1, 2, 0, 0, 1, 2, 0, 0, 1, 2, 1, 0, 2, 1, 0, 2, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 2, 1, 1, 1, 0, 1, 1, 1, 2, 2, 0, 1, 2, 2, 1,\n",
       "       0, 0, 1, 2, 2, 1, 2, 1, 1, 1, 0, 0, 2, 0, 2, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 0, 0, 1, 2, 2, 0, 1, 2, 2, 2, 2, 1,\n",
       "       0, 1, 0, 2, 0, 0, 1, 0, 0, 2, 1, 0, 2, 2, 0, 0, 2, 2, 2, 1, 1, 1,\n",
       "       1, 1, 1, 2, 0, 1, 1, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(outs).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d38a7791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0, -1, -1,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train-outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2dfcc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs= []\n",
    "for input, label in zip(test_input, y_test):\n",
    "    out = mlp(input.reshape(13,1))\n",
    "    # print(out)\n",
    "    outs.append(np.argmax(out))\n",
    "\n",
    "\n",
    "y_test-outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4640b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalization(batch, gamma, beta, epsilon=1e-5):\n",
    "    # Calculate mean and variance\n",
    "    mean = np.mean(batch, axis=0)\n",
    "    variance = np.var(batch, axis=0)\n",
    "    \n",
    "    # Normalize\n",
    "    batch_normalized = (batch - mean) / np.sqrt(variance + epsilon)\n",
    "    \n",
    "    # Scale and shift\n",
    "    batch_scaled_and_shifted = gamma * batch_normalized + beta\n",
    "    \n",
    "    return batch_scaled_and_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fc00f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "13cfaab314e57165490be6244dd780a39caf64916f8b5ba446816839af7ad0cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
