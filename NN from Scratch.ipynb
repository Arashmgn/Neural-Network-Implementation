{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03e073d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98bdd21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input  = np.array([[1],[1],[1],[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f48d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=weights.dot(inputs)+biases\n",
    "\n",
    "# x=x>0\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86d6fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.dot([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c8b03ba9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self,features,neurons,activationfunc):\n",
    "        self.features            = features\n",
    "        self.neurons             = neurons\n",
    "        self.weight              = np.random.randn(self.features,self.neurons)\n",
    "        self.bias                = np.random.randn(neurons,1)\n",
    "        self.input               = np.zeros([self.features,1])\n",
    "        self.output              = np.zeros([self.features,1])\n",
    "        self.activation          = np.zeros([self.features,1])\n",
    "\n",
    "        print(self.weight.shape,self.bias.shape)\n",
    "        self.activationfunc      = activationfunc\n",
    "        \n",
    "    def feedforward(self,input):\n",
    "        self.input = input\n",
    "        self.output              = self.weight.transpose().dot(input)+self.bias\n",
    "        self.output_activate     = self.activationFunction(self.output)  \n",
    "        return self.output_activate\n",
    "    \n",
    "    def activationReLu(self,inputs):\n",
    "        return np.maximum(inputs,0)\n",
    "        \n",
    "    def derivativeReLu(self,inputs):\n",
    "        return inputs > 0\n",
    "    \n",
    "    def activationSoftMax(self,inputs):\n",
    "        return np.exp(inputs)/np.sum(np.exp(inputs))\n",
    "        \n",
    "    def derivativeSofMax(self,inputs):\n",
    "        return inputs*(1-inputs)\n",
    "    \n",
    "    def activationFunction(self,inputs):\n",
    "        if self.activationfunc   == \"relu\":\n",
    "            return self.activationReLu(inputs)\n",
    "        elif self.activationfunc == \"softmax\":\n",
    "            return self.activationSoftMax(inputs)\n",
    "        else:\n",
    "            return inputs\n",
    "        \n",
    "    \n",
    "    def derivativeFunction(self):\n",
    "        if self.activationfunc   == \"relu\":\n",
    "            return self.derivativeReLu(self.output)\n",
    "        elif self.activationfunc == \"softmax\":\n",
    "            return self.derivativeSofMax(self.output_activate)\n",
    "        else :\n",
    "            return 1\n",
    "        \n",
    "    def backpropagation(self,error):       \n",
    "\n",
    "        # dEdZ=inputs*self.derivativeFunction()\n",
    "        # #derivative of loss for biases\n",
    "        # dEdB=dEdZ\n",
    "        # self.bias -= learning_rate*dEdB\n",
    "        # #derivative of loss for weights\n",
    "        # dEdW=dEdZ.dot(self.input.transpose())\n",
    "        # self.weight -= learning_rate*dEdW.transpose()\n",
    "        #derivative of loss for inputs, this will returned to previouse layer        \n",
    "        # dEdZ=self.weight.dot(inputs.transpose())*self.derivativeFunction()\n",
    "        # return self.weight.dot(dEdZ)\n",
    "\n",
    "        dEdZ = error*self.derivativeFunction()\n",
    "        dW   = np.dot(self.input, dEdZ.transpose())\n",
    "        dB   = dEdZ \n",
    "        dE   = np.dot(self.weight,  dEdZ)\n",
    "\n",
    "        return dW, dB, dE\n",
    "\n",
    "    \n",
    "    def learn(self, dW, dB, learning_rate):\n",
    "        self.weight -= dW*learning_rate\n",
    "        self.bias   -= dB*learning_rate\n",
    "\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_dim:int, layer_dims:list[int], activationfuncs:list[int], learning_rate=0.001):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.network = []\n",
    "        self.network.append(Layer(input_dim, layer_dims[0], activationfuncs[0]))\n",
    "        for i in range(len(layer_dims)):\n",
    "            try:\n",
    "                self.network.append(Layer(layer_dims[i], layer_dims[i+1], activationfuncs[i+1]))\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "\n",
    "    def feedforward(self, input):\n",
    "        out = input\n",
    "        for i in range(len(self.network)):\n",
    "            out = self.network[i].feedforward(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "    def backpropagation(self, ground_truth, output):\n",
    "        error = ground_truth - output\n",
    "\n",
    "        for i in range(len(self.network)):\n",
    "            dW, dB, error = self.network[i].backpropagetion(error)\n",
    "            self.network[i].learn(dW, dB, self.learning_rate)\n",
    "            \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "    def mse_loss(ground_truth, output):\n",
    "        return ((ground_truth - output)**2)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "350ff523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer  = Layer(4,10,\"relu\")\n",
    "# layer2 = Layer(10,5,\"relu\")\n",
    "# layer3 = Layer(5,2,\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10) (10, 1)\n",
      "(10, 5) (5, 1)\n",
      "(5, 2) (2, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.99999863e-01],\n",
       "       [1.37115077e-07]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP(4, layer_dims = [10, 5, 2], activationfuncs=[\"relu\", \"relu\", \"softmax\"], learning_rate=0.001)\n",
    "\n",
    "# mlp.feedforward(np.array([[1],[1],[1],[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "13cfaab314e57165490be6244dd780a39caf64916f8b5ba446816839af7ad0cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
